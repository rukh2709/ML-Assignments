{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "898169c5-885a-482d-a8ca-af98e896a4a1",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e8234-e966-4b22-8d3d-36c3a168525b",
   "metadata": {},
   "source": [
    "Answer 1: Lasso Regression is a linear regression technique that adds a regularization term to the cost function, which helps to shrink the coefficients of the input variables towards zero. This technique is particularly useful when there are many input variables, and some of them may not be relevant to the output.\n",
    "\n",
    "Compared to other regression techniques such as Ridge Regression or Ordinary Least Squares, Lasso Regression has the advantage of performing feature selection by shrinking the coefficients of less important variables to zero, making the model simpler and easier to interpret.\n",
    "\n",
    "Moreover, Lasso Regression can handle correlated variables better than Ridge Regression, as it tends to select only one variable from a group of highly correlated variables and set the coefficients of the rest to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570cfd1-35bf-4b05-9d5d-abb89b4267c4",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3af63-5385-4dcd-872e-c236949b55af",
   "metadata": {},
   "source": [
    "Answer 2: The main advantage of using Lasso Regression in feature selection is that it can effectively identify and select the most important features that contribute to the prediction of the output variable, while setting the coefficients of the less important features to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08afe2a5-f1c1-4004-8ba6-f13c894cd701",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc6b5f5-eea2-48eb-97d9-43d685732188",
   "metadata": {},
   "source": [
    "Answer 3: In Lasso Regression, the coefficients that are not shrunk to zero represent the most important features that contribute to the prediction of the output variable. The magnitude of the coefficients reflects the strength and direction of the relationship between each input variable and the output variable.\n",
    "\n",
    "The sign of the coefficient indicates whether the input variable has a positive or negative effect on the output variable. A positive coefficient means that an increase in the input variable will lead to an increase in the output variable, while a negative coefficient means that an increase in the input variable will lead to a decrease in the output variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bdbe4b-8387-4a3b-a127-2c9153c2a1ef",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b541b-d4a1-4c28-a2d9-6899ac76b86e",
   "metadata": {},
   "source": [
    "Answer 4: Lasso Regression has one tuning parameter, which is the regularization parameter, also known as alpha. \n",
    "\n",
    "When the alpha parameter is set to a low value, the model is more likely to overfit the training data and capture noise and irrelevant features, while when it is set to a high value, the model is more likely to underfit the data and miss important features that contribute to the output variable. Therefore, it's important to choose an appropriate value for alpha that balances between the bias and variance of the model and maximizes its performance on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541d143f-f3cf-4f8f-a8b6-acb5dc1fcc09",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c1ab2a-dc17-45ee-9009-8e5be7e4630d",
   "metadata": {},
   "source": [
    "Answer 5: Yes, Lasso Regression can be used for non-linear regression problems by incorporating non-linear transformations of the input variables in the model.\n",
    "\n",
    "One common approach is to use polynomial features, which involves creating higher-order terms of the input variables (e.g., squares, cubes, interactions) and adding them to the model as new features. This can capture non-linear relationships between the input variables and the output variable, and allow Lasso Regression to fit non-linear patterns in the data.\n",
    "\n",
    "For instance, if we have a single input variable x and want to fit a quadratic function of the form y = a + bx + cx^2, we can create a new feature x^2 and add it to the model as a new input variable. Then, Lasso Regression can be applied to the extended dataset with two input variables (x and x^2), and the regularization term will shrink the coefficients of the less important features towards zero, effectively performing feature selection and improving the generalization performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bebaa3-7ff3-4871-bb49-bfb8e0e64bc4",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8783cb3-0f43-4b9c-9d28-61476178c63a",
   "metadata": {},
   "source": [
    "Answer 6: The main difference between Ridge Regression and Lasso Regression is that Ridge Regression uses an L2 regularization term, which adds the sum of squares of the coefficients to the loss function, while Lasso Regression uses an L1 regularization term, which adds the sum of absolute values of the coefficients to the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b499783-9197-47ce-92d1-6e090d0be40e",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec0a50-ece1-44ed-89a0-6c4335fc3e9e",
   "metadata": {},
   "source": [
    "Answer 7: Yes, Lasso Regression can handle multicollinearity in the input features, but it does so differently than Ridge Regression.\n",
    "\n",
    "Multicollinearity occurs when two or more input variables in a regression model are highly correlated, which can lead to unstable and unreliable estimates of the coefficients. In Lasso Regression, the L1 regularization term can help to mitigate the effects of multicollinearity by shrinking the coefficients of the correlated input variables towards zero, effectively selecting one of them and excluding the others from the model. This is because the L1 regularization term tends to produce sparse solutions where only a subset of the input variables has non-zero coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d737e08-c888-4a2d-963c-0e6acaa3f359",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e045fc51-a91f-402c-b2d2-aadfa286bed6",
   "metadata": {},
   "source": [
    "Answer 8: The optimal value of the regularization parameter (lambda) in Lasso Regression can be chosen through cross-validation on the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
