{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dc8c8bb-d6da-4d41-b08f-22c98a2397e9",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8a013-6874-4f53-8b34-be08e5d0c354",
   "metadata": {},
   "source": [
    "Answer 1: Gradient Boosting Regression is a type of boosting algorithm that is used for regression problems. It is based on the idea of building an ensemble of weak regression models, such as decision trees, and combining their predictions to create a stronger model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f23706-390a-4fde-9f3f-4411779ed835",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb91cf-034e-4e50-98ba-dcc721da50ec",
   "metadata": {},
   "source": [
    "Answer 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014f96b2-4515-4d09-b0ec-9a32642ff277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.4203\n",
      "R-Squared: -0.2366\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.init_prediction = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.init_prediction = np.mean(y)\n",
    "        y_pred = np.full_like(y, self.init_prediction)\n",
    "        for i in range(self.n_estimators):\n",
    "            # Compute the residual\n",
    "            residual = y - y_pred\n",
    "            \n",
    "            # Fit a decision tree to the residual\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            model.fit(X, residual)\n",
    "            \n",
    "            # Update the predictions\n",
    "            y_pred += self.learning_rate * model.predict(X)\n",
    "            \n",
    "            # Save the model\n",
    "            self.models.append(model)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(X.shape[0], self.init_prediction)\n",
    "        for model in self.models:\n",
    "            y_pred += self.learning_rate * model.predict(X)\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        return mse, r2\n",
    "    \n",
    "# Generate a small dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 5)\n",
    "y = np.sum(X, axis=1) + np.random.randn(100)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_X, train_y = X[:80], y[:80]\n",
    "test_X, test_y = X[80:], y[80:]\n",
    "\n",
    "# Fit the model to the training set\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(train_X, train_y)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "mse, r2 = gb.score(test_X, test_y)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-Squared: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe425b-4b0a-4d02-a68d-7a173ea1011b",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c89e299-802b-42d0-9d1a-0cddc498cc65",
   "metadata": {},
   "source": [
    "Answer 3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28cd9c6b-1de5-48b2-bc97-ed62e3e5fc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_estimators': 200, 'learning_rate': 0.5, 'max_depth': 4}\n",
      "Mean Squared Error: 5.7414\n",
      "R-Squared: -1.9334\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.init_prediction = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.init_prediction = np.mean(y)\n",
    "        y_pred = np.full_like(y, self.init_prediction)\n",
    "        for i in range(self.n_estimators):\n",
    "            # Compute the residual\n",
    "            residual = y - y_pred\n",
    "            \n",
    "            # Fit a decision tree to the residual\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            model.fit(X, residual)\n",
    "            \n",
    "            # Update the predictions\n",
    "            y_pred += self.learning_rate * model.predict(X)\n",
    "            \n",
    "            # Save the model\n",
    "            self.models.append(model)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(X.shape[0], self.init_prediction)\n",
    "        for model in self.models:\n",
    "            y_pred += self.learning_rate * model.predict(X)\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        return mse, r2\n",
    "    \n",
    "    def grid_search(self, X, y, param_grid):\n",
    "        best_params = None\n",
    "        best_score = float(\"inf\")\n",
    "        for params in product(*param_grid.values()):\n",
    "            params = dict(zip(param_grid.keys(), params))\n",
    "            self.__init__(**params)\n",
    "            self.fit(X, y)\n",
    "            score = self.score(X, y)[0]\n",
    "            if score < best_score:\n",
    "                best_params = params\n",
    "                best_score = score\n",
    "        self.__init__(**best_params)\n",
    "        self.fit(X, y)\n",
    "        return best_params\n",
    "    \n",
    "# Define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.5],\n",
    "    \"max_depth\": [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "gb = GradientBoostingRegressor()\n",
    "best_params = gb.grid_search(train_X, train_y, param_grid)\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "# Fit the model to the training set using the best hyperparameters\n",
    "gb.fit(train_X, train_y)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "mse, r2 = gb.score(test_X, test_y)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-Squared: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5cdf5f-6553-4a7f-a732-faa4f89bff28",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b91a6fd-a620-443c-b3ee-f26e4690bb90",
   "metadata": {},
   "source": [
    "Answer 4: A weak learner in gradient boosting is a simple model that is used to fit the residuals of the previous model in the sequence. In gradient boosting, the objective is to iteratively improve the predictions by combining the predictions of many weak learners. A weak learner can be any model that performs better than random guessing, such as a decision tree with low depth or a linear regression model. The key characteristic of a weak learner is that it should be simple and fast to train, since many weak learners are typically used in the sequence. In contrast, a strong learner is a model that can accurately predict the target variable on its own, without the need for boosting or ensembling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb47d65-8fa8-40a6-84fa-8574abc13e9b",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5836e6-d369-4ce2-85d1-4050806dbdd8",
   "metadata": {},
   "source": [
    "Answer 5: The intuition behind the gradient boosting algorithm is to iteratively add simple models (weak learners) to the ensemble, each one correcting the errors made by the previous models in the sequence. The key idea is to fit the residuals of the previous model instead of the original target variable. This way, the new model focuses on the errors made by the previous models and tries to minimize them. The process continues until the desired number of models is reached, or until the performance on the validation set stops improving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e387a3-b3bf-460c-a1ac-f72b0e145304",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786867da-7cea-4283-804b-815965d774c2",
   "metadata": {},
   "source": [
    "Answer 6: The Gradient Boosting algorithm builds an ensemble of weak learners by iteratively adding them to the ensemble, each one correcting the errors made by the previous models in the sequence. following are the main steps of the algorithm:\n",
    "\n",
    "Initialize the ensemble with a constant prediction value, usually the mean or median of the target variable.\n",
    "\n",
    "For each iteration:\n",
    "\n",
    "a. Compute the negative gradient of the loss function with respect to the current predictions. This represents the direction of steepest descent, which indicates how the predictions should be updated to reduce the loss.\n",
    "\n",
    "b. Train a weak learner on the negative gradient, which effectively makes it a direct correction of the previous model's errors.\n",
    "\n",
    "c. Add the new model to the ensemble, weighted by a learning rate parameter. The learning rate controls the contribution of each model to the final prediction, and it is used to prevent overfitting and to speed up convergence.\n",
    "\n",
    "Repeat steps 2 until the desired number of models is reached, or until the performance on the validation set stops improving.\n",
    "\n",
    "To make a prediction for a new instance, compute the weighted sum of the predictions from all the models in the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42008c80-bb26-4462-bd20-44bdf774c4c5",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7e7fc-2f29-480c-8b93-7523afcee433",
   "metadata": {},
   "source": [
    "Answer 7: The mathematical intuition behind the Gradient Boosting algorithm can be broken down into the following steps:\n",
    "\n",
    "Define the loss function: The first step in constructing the mathematical intuition of Gradient Boosting is to define a differentiable loss function that measures the difference between the predicted values and the actual target values.\n",
    "\n",
    "Initialize the predictions: The algorithm starts by initializing the predictions with a constant value, such as the mean or median of the target variable.\n",
    "\n",
    "Compute the negative gradient: The negative gradient of the loss function with respect to the current predictions is computed, which represents the direction of steepest descent. This gradient indicates how the predictions should be updated to minimize the loss function.\n",
    "\n",
    "Train a weak learner: A weak learner, such as a decision tree, is trained to predict the negative gradient of the loss function. The aim is to fit the residuals of the previous model, rather than the original target variable.\n",
    "\n",
    "Update the predictions: The predictions are updated by adding the weighted predictions of the weak learner to the current predictions. The weights are controlled by a learning rate parameter, which prevents overfitting and slows down the learning process.\n",
    "\n",
    "Repeat steps 3-5: Steps 3-5 are repeated iteratively, each time fitting the residuals of the previous model and updating the predictions accordingly. This results in an ensemble of weak learners that work together to make accurate predictions.\n",
    "\n",
    "Make predictions: To make a prediction for a new instance, the predictions from all the weak learners in the ensemble are combined using their respective weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
